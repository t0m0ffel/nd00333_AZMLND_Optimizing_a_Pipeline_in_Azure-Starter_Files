# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

The dataset contains information about a banks marketing campaign. We are interested whether the potential customers responded positively to the marketing efforts

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

The best performing model was found by AutoML, used VotingEnsemble and has an accuracy of **0.9175**

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

We used the above-mentioned data set which contains 21 columns (including the target column). Features can be categorical, numeric or binary. The target is a binary variable.
For classification, we used scikit-learns implementation of Logistic Regression which is suited for a binary classification task. We tuned the  hyperparameters regularization strength and max iterations using hyperdrive 

The best performing logistic regression model used max iterations of **200** and a regularization strength of **0.378**. The accuracy was **0.913**.


**What are the benefits of the parameter sampler you chose?**

We can test a range of hyperparameters without searching the entire space which is computational expensive and time-consuming. This ensures that we use better hyperparameters than the default ones in our experiment.

**What are the benefits of the early stopping policy you chose?**

Bandit terminates runs which are not within the range of a slack factor compared to best run. This helps to decrease computing cost and time consumption.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

In our training we used the following hyperparameters:
- **task: classification** 
  - since we want to predict a binary variable (e.g. predict class "yes" or "no") this is the appropriate choice here 
- **iterations: 10**
  - we use relatively few iterations (the default is 1000) here since this is a demo project, and we are time-constrained since we are using the provided virtual machine 
- **iteration_timeout_minutes: 2**
  - Since we use a small data set 2 minutes is generous. As we see in the training no model trains longer than 1 minute.
- **primary_metric: accuracy**
  - We use the default parameter here. Other metrics such as *AUC_weighted* might be appropriated as well since it is better suited for class imbalance.
- **n_cross_validations: 3**
  - We use the default value here.
- **experiment_timeout_minutes: 30**
  - This ensures the experiment stops before out virtual machine stops.

Please refer to the full documentation [here](https://learn.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig?view=azure-ml-py).

AutoML found VotingEnsemble working best with an accuracy of **0.9175**. 
The following models where used as voters: XGBoostClassifier, LightGBM, XGBoostClassifier, XGBoostClassifier, XGBoostClassifier, LightGBM, XGBoostClassifier, XGBoostClassifier, LogisticRegression, SGD

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The two models performed as expected very similarly with AutoMLs model performing 0.4% better. 
Since the difference is so small I suspect that AutoML might perform worse when the experiment is repeated with the same parameters.
But it has to be noted that both models performed good on the dataset so the room for improvement was limited.


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

**Class imbalance:** Since the target labels are very imbalanced (8872 no and 1128 yes) we could use the techniques of over or under sampling to improve our model. These are proven methods which can deal with exactly this problem.

**Fixing azure kernel:** The provided kernel was not able to utilize AutoML unless we executed the following commands:
```
conda activate azureml_py38
pip install certifi==2022.9.24
sudo chmod 755 /anaconda/envs/azureml_py38/lib/python3.8/site-packages/nimbusml/internal/libs/pybridge.so
```
This should be fixed so that AutoML works out of the box.




## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**

I did it in the code.
